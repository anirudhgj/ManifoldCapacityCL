{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold analysis example with RESNET\n",
    "\n",
    "\n",
    "This notebook contains a short example for running an analysis on a PyTorch model. For this type of analysis, we should use a dataset with a large number of classes, roughly `num_classes>30`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some generic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "from torchvision import datasets, transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to import the `manifold_analysis_corr` function which implements the analysis technique. In addition, we'll also import some helper functions for creating the appropriate input datasets (`make_manifold_data`) and extracting model activations (`extractor`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models.resnet34\n",
    "# models.vgg16\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mftma.manifold_analysis_correlation import manifold_analysis_corr\n",
    "from mftma.utils.make_manifold_data import make_manifold_data\n",
    "from mftma.utils.activation_extractor import extractor\n",
    "from mftma.utils.analyze_pytorch import analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model\n",
    "\n",
    "We also need a model and a dataset, so we'll quickly train an insance of VGG16 on CIFAR-100 for a few epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create the datasets and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ../data/cifar-100-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 169001437/169001437 [00:02<00:00, 76626765.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/cifar-100-python.tar.gz to ../data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "mean = (0.5070751592371323, 0.48654887331495095, 0.4409178433670343)\n",
    "std = (0.2673342858792401, 0.2564384629170883, 0.27615047132568404)\n",
    "\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = datasets.CIFAR100(\n",
    "    \"../data\", train=True, download=True, transform=transform_train\n",
    ")\n",
    "test_dataset = datasets.CIFAR100(\n",
    "    \"../data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor(), transforms.Normalize(mean, std)]\n",
    "    ),\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1024, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        output = torch.nn.functional.log_softmax(output, dim=1)\n",
    "        loss = torch.nn.functional.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            output = torch.nn.functional.log_softmax(output, dim=1)\n",
    "            test_loss += torch.nn.functional.nll_loss(\n",
    "                output, target, reduction=\"sum\"\n",
    "            ).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print(\n",
    "        \"\\nTest set epoch {}: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n\".format(\n",
    "            epoch,\n",
    "            test_loss,\n",
    "            correct,\n",
    "            len(test_loader.dataset),\n",
    "            100.0 * correct / len(test_loader.dataset),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's train the model for a few epochs. As this is just an example, we don't care about making the best performing model we can here. It is easy to make a much better model if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set epoch 0: Average loss: 4.1934, Accuracy: 677/10000 (7%)\n",
      "\n",
      "\n",
      "Test set epoch 1: Average loss: 3.7547, Accuracy: 1262/10000 (13%)\n",
      "\n",
      "\n",
      "Test set epoch 2: Average loss: 3.5496, Accuracy: 1613/10000 (16%)\n",
      "\n",
      "\n",
      "Test set epoch 3: Average loss: 3.4132, Accuracy: 1839/10000 (18%)\n",
      "\n",
      "\n",
      "Test set epoch 4: Average loss: 3.2880, Accuracy: 2003/10000 (20%)\n",
      "\n",
      "\n",
      "Test set epoch 5: Average loss: 3.2309, Accuracy: 2138/10000 (21%)\n",
      "\n",
      "\n",
      "Test set epoch 6: Average loss: 3.1248, Accuracy: 2410/10000 (24%)\n",
      "\n",
      "\n",
      "Test set epoch 7: Average loss: 3.0422, Accuracy: 2576/10000 (26%)\n",
      "\n",
      "\n",
      "Test set epoch 8: Average loss: 3.0232, Accuracy: 2554/10000 (26%)\n",
      "\n",
      "\n",
      "Test set epoch 9: Average loss: 2.9669, Accuracy: 2657/10000 (27%)\n",
      "\n",
      "\n",
      "Test set epoch 10: Average loss: 2.9356, Accuracy: 2765/10000 (28%)\n",
      "\n",
      "\n",
      "Test set epoch 11: Average loss: 2.8755, Accuracy: 2911/10000 (29%)\n",
      "\n",
      "\n",
      "Test set epoch 12: Average loss: 2.8442, Accuracy: 2930/10000 (29%)\n",
      "\n",
      "\n",
      "Test set epoch 13: Average loss: 2.7932, Accuracy: 3046/10000 (30%)\n",
      "\n",
      "\n",
      "Test set epoch 14: Average loss: 2.7577, Accuracy: 3096/10000 (31%)\n",
      "\n",
      "\n",
      "Test set epoch 15: Average loss: 2.7623, Accuracy: 3150/10000 (32%)\n",
      "\n",
      "\n",
      "Test set epoch 16: Average loss: 2.7224, Accuracy: 3228/10000 (32%)\n",
      "\n",
      "\n",
      "Test set epoch 17: Average loss: 2.7123, Accuracy: 3264/10000 (33%)\n",
      "\n",
      "\n",
      "Test set epoch 18: Average loss: 2.7271, Accuracy: 3281/10000 (33%)\n",
      "\n",
      "\n",
      "Test set epoch 19: Average loss: 2.6948, Accuracy: 3324/10000 (33%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "model = models.resnet34(num_classes=100)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "for i in range(20):\n",
    "    train(model, device, train_loader, optimizer)\n",
    "    test(model, device, test_loader, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the model to eval mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the manifold dataset\n",
    "\n",
    "To create this, we have to specify the number of classes we want to sample, which in this case will just be the total number of samples in the dataset, so `sampled_classes=100`. We also need to decide how many examples per class we want to use, and in this case we will use `examples_per_class=50`. Note that using large numbers of examples will result in a much longer runtime. We will also create the manifold data from the train dataset, and show the test dataset later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_classes = 100\n",
    "examples_per_class = 50\n",
    "\n",
    "data = make_manifold_data(train_dataset, sampled_classes, examples_per_class, seed=0)\n",
    "data = [d.to(device) for d in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 3, 32, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract activations from the model\n",
    "\n",
    "Now we need to extract the activations at each layer of the model when the manifold data is given as an input. We can use the extractor given in `mftma.utils.activation_extractor`, which *usually* works, though depending on how the specific model is implemented, might miss some layers. If you do use it, make sure that all the layers you want to analyze are found!  For this example, we will only look at the `Conv2D` and `Linear` layers of VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['layer_0_Input',\n",
       " 'layer_1_Conv2d',\n",
       " 'layer_5_Conv2d',\n",
       " 'layer_8_Conv2d',\n",
       " 'layer_10_Conv2d',\n",
       " 'layer_13_Conv2d',\n",
       " 'layer_15_Conv2d',\n",
       " 'layer_18_Conv2d',\n",
       " 'layer_20_Conv2d',\n",
       " 'layer_23_Conv2d',\n",
       " 'layer_25_Conv2d',\n",
       " 'layer_27_Conv2d',\n",
       " 'layer_30_Conv2d',\n",
       " 'layer_32_Conv2d',\n",
       " 'layer_35_Conv2d',\n",
       " 'layer_37_Conv2d',\n",
       " 'layer_40_Conv2d',\n",
       " 'layer_42_Conv2d',\n",
       " 'layer_45_Conv2d',\n",
       " 'layer_47_Conv2d',\n",
       " 'layer_49_Conv2d',\n",
       " 'layer_52_Conv2d',\n",
       " 'layer_54_Conv2d',\n",
       " 'layer_57_Conv2d',\n",
       " 'layer_59_Conv2d',\n",
       " 'layer_62_Conv2d',\n",
       " 'layer_64_Conv2d',\n",
       " 'layer_67_Conv2d',\n",
       " 'layer_69_Conv2d',\n",
       " 'layer_72_Conv2d',\n",
       " 'layer_74_Conv2d',\n",
       " 'layer_77_Conv2d',\n",
       " 'layer_79_Conv2d',\n",
       " 'layer_81_Conv2d',\n",
       " 'layer_84_Conv2d',\n",
       " 'layer_86_Conv2d',\n",
       " 'layer_89_Conv2d',\n",
       " 'layer_92_Linear']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations = extractor(model, data, layer_types=[\"Conv2d:1\", \"Linear\"])\n",
    "list(activations.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare activations for analysis\n",
    "\n",
    "Now we're almost ready to run the analysis on the extracted activations. The final step is to convert them into the correct shape and pass them to `manifold_analysis_corr`. For example, the shape of the activations in `layer_1_Conv2d` is `(50, 64, 32, 32)`, which we will flatten to `(50, 65536)` and transpose to the `(65536, 50)` format the analysis expects.  This flattening may not always be appropriate as one might want to analyze each spatial location (or each timestep of a sequence model) independently.\n",
    "\n",
    "Additionally, the number of features here is quite a bit larger than needed, so we'll also do a random projection to `5000` dimensions to save time and memory usage. This step is optional and shouldn't change the geometry too much (see the Johnson–Lindenstrauss lemma) but it is useful to save on computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projecting layer_1_Conv2d\n"
     ]
    }
   ],
   "source": [
    "for layer, data in activations.items():\n",
    "    X = [d.reshape(d.shape[0], -1).T for d in data]\n",
    "    # Get the number of features in the flattened data\n",
    "    N = X[0].shape[0]\n",
    "    # If N is greater than 5000, do the random projection to 5000 features\n",
    "    if N > 5000:\n",
    "        print(\"Projecting {}\".format(layer))\n",
    "        M = np.random.randn(5000, N)\n",
    "        M /= np.sqrt(np.sum(M * M, axis=1, keepdims=True))\n",
    "        X = [np.matmul(M, d) for d in X]\n",
    "    activations[layer] = X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the analysis on the prepped activations\n",
    "\n",
    "And store the results for later plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer_0_Input capacity: 0.040464, radius 1.551578, dimension 34.585561, correlation 0.329898\n",
      "layer_1_Conv2d capacity: 0.044254, radius 1.519446, dimension 32.060822, correlation 0.386749\n",
      "layer_5_Conv2d capacity: 0.045726, radius 1.459642, dimension 31.743724, correlation 0.344369\n",
      "layer_8_Conv2d capacity: 0.045333, radius 1.442606, dimension 32.185682, correlation 0.326193\n",
      "layer_10_Conv2d capacity: 0.044917, radius 1.453493, dimension 32.330327, correlation 0.322394\n",
      "layer_13_Conv2d capacity: 0.044292, radius 1.480990, dimension 32.784761, correlation 0.333286\n",
      "layer_15_Conv2d capacity: 0.045433, radius 1.439029, dimension 32.171354, correlation 0.326274\n",
      "layer_18_Conv2d capacity: 0.043744, radius 1.456832, dimension 33.174038, correlation 0.296310\n",
      "layer_20_Conv2d capacity: 0.045882, radius 1.438177, dimension 31.923844, correlation 0.309203\n"
     ]
    }
   ],
   "source": [
    "capacities = []\n",
    "radii = []\n",
    "dimensions = []\n",
    "correlations = []\n",
    "\n",
    "for k, X in activations.items():\n",
    "    # Analyze each layer's activations\n",
    "    a, r, d, r0, K = manifold_analysis_corr(X, 0, 300, n_reps=1)\n",
    "\n",
    "    # Compute the mean values\n",
    "    a = 1 / np.mean(1 / a)\n",
    "    r = np.mean(r)\n",
    "    d = np.mean(d)\n",
    "    print(\n",
    "        \"{} capacity: {:4f}, radius {:4f}, dimension {:4f}, correlation {:4f}\".format(\n",
    "            k, a, r, d, r0\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Store for later\n",
    "    capacities.append(a)\n",
    "    radii.append(r)\n",
    "    dimensions.append(d)\n",
    "    correlations.append(r0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the results\n",
    "\n",
    "Here we will plot the results of the analysis we just ran. Note that we won't plot the results of the final linear layer as this is the model output after classification has already occured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(18, 4))\n",
    "\n",
    "axes[0].plot(capacities, linewidth=5)\n",
    "axes[1].plot(radii, linewidth=5)\n",
    "axes[2].plot(dimensions, linewidth=5)\n",
    "axes[3].plot(correlations, linewidth=5)\n",
    "\n",
    "axes[0].set_ylabel(r\"$\\alpha_M$\", fontsize=18)\n",
    "axes[1].set_ylabel(r\"$R_M$\", fontsize=18)\n",
    "axes[2].set_ylabel(r\"$D_M$\", fontsize=18)\n",
    "axes[3].set_ylabel(r\"$\\rho_{center}$\", fontsize=18)\n",
    "\n",
    "names = list(activations.keys())\n",
    "names = [n.split(\"_\")[1] + \" \" + n.split(\"_\")[2] for n in names]\n",
    "for ax in axes:\n",
    "    ax.set_xticks([i for i, _ in enumerate(names)])\n",
    "    ax.set_xticklabels(names, rotation=90, fontsize=16)\n",
    "    ax.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined analysis\n",
    "\n",
    "The above steps are also bundled together in the `analyze` function in `utils.analyze_pytorch`. This works for some use cases, but more complex analyses may require more control. Here, we will demonstrate this on the CIFAR-100 test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_classes = 100\n",
    "# examples_per_class = 50\n",
    "# layer_types = [\"Input\", \"Conv2d\", \"Linear\"]\n",
    "# projection = True\n",
    "# projection_dimension = 5000\n",
    "# seed = 0\n",
    "\n",
    "# model = model.eval()\n",
    "# results = analyze(\n",
    "#     model,\n",
    "#     test_dataset,\n",
    "#     sampled_classes=sampled_classes,\n",
    "#     examples_per_class=examples_per_class,\n",
    "#     layer_types=layer_types,\n",
    "#     projection=projection,\n",
    "#     projection_dimension=projection_dimension,\n",
    "#     seed=seed,\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# capacities = []\n",
    "# radii = []\n",
    "# dimensions = []\n",
    "# correlations = []\n",
    "# for layer, result in results.items():\n",
    "#     a = 1 / np.mean(1 / result[\"capacity\"])\n",
    "#     r = np.mean(result[\"radius\"])\n",
    "#     d = np.mean(result[\"dimension\"])\n",
    "\n",
    "#     capacities.append(a)\n",
    "#     radii.append(r)\n",
    "#     dimensions.append(d)\n",
    "#     correlations.append(result[\"correlation\"])\n",
    "\n",
    "# fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# axes[0].plot(capacities, linewidth=5)\n",
    "# axes[1].plot(radii, linewidth=5)\n",
    "# axes[2].plot(dimensions, linewidth=5)\n",
    "# axes[3].plot(correlations, linewidth=5)\n",
    "\n",
    "# axes[0].set_ylabel(r\"$\\alpha_M$\", fontsize=18)\n",
    "# axes[1].set_ylabel(r\"$R_M$\", fontsize=18)\n",
    "# axes[2].set_ylabel(r\"$D_M$\", fontsize=18)\n",
    "# axes[3].set_ylabel(r\"$\\rho_{center}$\", fontsize=18)\n",
    "\n",
    "# names = list(results.keys())\n",
    "# names = [n.split(\"_\")[1] + \" \" + n.split(\"_\")[2] for n in names]\n",
    "# for ax in axes:\n",
    "#     ax.set_xticks([i for i, _ in enumerate(names)])\n",
    "#     ax.set_xticklabels(names, rotation=90, fontsize=16)\n",
    "#     ax.tick_params(axis=\"both\", which=\"major\", labelsize=14)\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
